version: '3'

env:
  MONITOR_NAMESPACE: observe
  GCP_PROJECT: gcplocal-emulator
  GCP_LOCAL_BASE_URL: http://localhost:8085/v1
  DEV_PATH: ./k8s/dev
  PROD_PATH: ./k8s/prod
  

tasks:
  # / ======== \
  #   region DEV  
  # \ ======== /
  _run-local:
    dir: ./scripts
    desc: setup & run everything locally!
    cmds:
      -  bash -c "./run-local.sh"

  _stop-local:
    dir: ./scripts
    desc: stop the K3s deployment & containers running locally (for the proj)
    cmds:
      -  bash -c "./stop-local.sh"

  ns-create:
    desc: create all required namespaces
    cmds:
      - kubectl create ns observe
      - kubectl create ns verne
      
  observe-grafana-access-local:
    desc: access the grafana web ui (get address & details)
    cmds: 
      - |
        GRAFANA_IP=$(kubectl get svc grafana -n observe -o jsonpath='{.spec.clusterIP}')
        GRAFANA_PORT=3045
        GRAFANA_PASS=$(kubectl get secret grafana -n observe -o jsonpath="{.data.admin-password}" | base64 --decode)
        echo "Grafana Dashboard URL: http://${GRAFANA_IP}:${GRAFANA_PORT}"
        echo "Username: admin"
        echo "Password: ${GRAFANA_PASS}"
 
  observe-loki-update-local:
    dir: $DEV_PATH/observe
    desc: upgrade the loki helm pods on observe ns
    cmds:
      - helm upgrade loki grafana/loki -n $MONITOR_NAMESPACE -f ./values/loki.yaml

  observe-prom-update-local:
    dir: $DEV_PATH/observe
    desc: upgrade the prometheus pods on observe ns
    cmds:
      - helm upgrade prometheus prometheus-community/prometheus -n $MONITOR_NAMESPACE -f ./values/prom.yaml
  
  verne-helm-pack-install:
    desc: install verne helm package
    cmds:
      - helm install verne verne/verne -n verne

  verne-get-ip:
    desc: get verne broker load balancer IP to access 
    cmds:
      - kubectl get svc vernemq-broker -n verne -o jsonpath='{.spec.clusterIP}' 

  _run-k6-test:
    dir: ./scripts/
    desc: load test verneMQTT k3s with k6 [ARGS- duration(secs), vus]
    cmds:
      - bash -c "./k6-run.sh {{.CLI_ARGS}}"

  k6-build:
    dir: ./services/k6
    desc: rebuild the k6 source image from Dockerfile
    cmds:
      - docker build --no-cache -t k6-verne:latest

  k6-image-push:  
    desc: push the k6 verne image to dockerhub [ARGS- version]
    cmds:
      - docker tag k6-verne:latest sardinesszsz/k6-verne:{{.CLI_ARGS}}
      - docker push sardinesszsz/k6-verne:{{.CLI_ARGS}}
    
  k6-github-act-local:
    dir: ../
    desc: run & test github actions CI/CD work locally to push K6 image to dockerhub (dependency - act)
    cmds:
      - act -P act -P ubuntu-latest=catthehacker/ubuntu:act-latest --secret-file .github.env --bind /var/run/docker.sock:/var/run/docker.sock -j build-k6-local

  listener-build:
    dir: ./services/listener
    desc: rebuild the listener source image from Dockerfile
    cmds:
      - docker build --no-cache -t verne-listener:latest .

  listener-push:
    desc: push the listener image to dockerhub [ARGS- version]
    cmds: 
      - docker tag verne-listener:latest sardinesszsz/verne-listener:{{.CLI_ARGS}}
      - docker push sardinesszsz/verne-listener:{{.CLI_ARGS}}

  listener-github-act-local:
    dir: ../
    desc: run & test github actions CI/CD work locally to push lustener image to dockerhub (dependency - act)
    cmds:
      - act -P act -P ubuntu-latest=catthehacker/ubuntu:act-latest --secret-file .github.env --bind /var/run/docker.sock:/var/run/docker.sock -j build-listener-local

  directrunner-pipeline-run:
    dir: ./services/gcp/pipeline
    desc: beam directrunner local pipeline (pubsub -> beam -> bigtable)
    cmds:
      - zsh -i -c "conda activate logcore && python ./pipeline.py --project=$GCP_PROJECT --topic=$GCP_TOPIC --host=$GCP_HOST"

  directrunner-pipeline-view-logs:
    dir: ./services/gcp/pipeline
    desc: read logfile of beam directrunner local (start, stop, messages, etc)
    cmds: 
      - bash -c "tail -f ./pipeline.log"

  gcp-local-run:
    dir: ./services/gcp
    desc: run the gcp local emulator 
    cmds:
      - COMPOSE_BAKE=true docker compose up -d
  
  gcp-local-remove:
    dir: ./services/gcp
    desc: take down gcp local emulator
    cmds: 
      - docker compose down

  gcp-local-init:
    desc: initialize gcp local pubsub(topics) & bigtable(relations)
    cmds:
      - bash -c "./scripts/pubsub-create-topic.sh"
      - zsh -i -c "conda activate logcore && python ./services/gcp/bigtable/init.py"
  
  gcp-local-bigtable-get-relations-list:
    dir: ./services/gcp/bigtable
    desc: get list of test relations on bigtable(local)
    cmds:
      - zsh -i -c "conda activate logcore && python list-relations.py"

  gcp-local-bigtable-query:
    dir: ./services/gcp/bigtable
    desc: get list of test relations on bigtable(local) [ARGS- relation]
    cmds:
      - zsh -i -c "conda activate logcore && python query.py --relation {{.CLI_ARGS}}"

  gcp-local-bigtable-clear-relation:
    dir: ./services/gcp/bigtable
    desc: clear all entries of relations on bigtable(local) [ARGS- relation]
    cmds:
      - zsh -i -c "conda activate logcore && python clear.py --relation {{.CLI_ARGS}}"
 
  gcp-local-pubsub-list-topic:
    desc: delete a specific pubsub topic  
    cmds:
      - curl -X GET "$GCP_LOCAL_BASE_URL/projects/$GCP_PROJECT/topics" 

  gcp-local-pubsub-listen-source:
    dir: ./services/gcp/pubsub
    desc: subsribe and listen to pubsub source(local)
    cmds: 
      - zsh -i -c "conda activate logcore && python sub.py" 

  
      
  # / ======== \
  #   region PROD 
  # \ ======== /
