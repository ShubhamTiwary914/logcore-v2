version: '3'

dotenv: ['.tf.env']


env:
  MONITOR_NAMESPACE: observe
  GCP_PROJECT: gcplocal-emulator
  GCP_LOCAL_BASE_URL: http://localhost:8085/v1
  DEV_PATH: ./k8s/dev
  PROD_PATH: ./k8s/prod

tasks:
  testcmd:
    cmds:
      - |
        set -- {{.CLI_ARGS}}
        arg1=$1
        arg2=$2
        arg3=$3
        echo command --flag1 "$arg1" --flag2 "$arg2" --flag3 "$arg3"

  # / ======== \
  #   region DEV  
  # \ ======== /
  _run-local:
    dir: ./scripts
    desc: setup & run everything locally!
    cmds:
      -  bash -c "./run-local.sh"

  _stop-local:
    dir: ./scripts
    desc: stop the K3s deployment & containers running locally (for the proj)
    cmds:
      -  bash -c "./stop-local.sh"

  ns-create:
    desc: create all required namespaces
    cmds:
      - kubectl create ns observe
      - kubectl create ns verne
      
  helm-update-repos:
    desc: pull & update all changed helm repo releases
    cmds:
      - helm repo add verne https://raw.githubusercontent.com/ShubhamTiwary914/logcore-v2/charts/ 2>/dev/null || echo "Verne repo already added"
      - helm repo add grafana https://grafana.github.io/helm-charts 2>/dev/null || echo "Grafana repo already added"
      - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts 2>/dev/null || echo "Prometheus repo already added"
      - helm repo update
      
  helm-view-dev-manifest:
    desc: view how rendered verne manifests will be after applying dev values
    cmds: 
      - mkdir -p templates
      - helm template verne ./helm/verne  --values ./k8s/dev/verne/dev-values.yaml --debug > ./templates/dev-verne-manifest.yaml
      
  helm-view-prod-manifest:
    desc: view how rendered verne manifests will be after applying prod values
    cmds: 
      - mkdir -p templates
      - helm template verne ./helm/verne  --values ./k8s/prod/verne/prod-values.yaml --debug > ./templates/prod-verne-manifest.yaml

  helm-view-observe-dev-manifests:
    desc: view dev observe manifests for alloy, prom, graf, loki
    cmds:
      - mkdir -p templates/observe
      - helm template alloy grafana/alloy --values ./k8s/dev/observe/values/alloy.yaml --debug > ./templates/observe/alloy-dev-manifest.yaml
      - helm template grafana grafana/grafana --values ./k8s/dev/observe/values/graf.yaml --debug > ./templates/observe/graf-dev-manifest.yaml
      - helm template loki grafana/loki --values ./k8s/dev/observe/values/loki.yaml --debug > ./templates/observe/loki-dev-manifest.yaml
      - helm template prom prometheus-community/prometheus --values ./k8s/dev/observe/values/prom.yaml --debug > ./templates/observe/prom-dev-manifest.yaml
      

  observe-grafana-access-local:
    desc: access the grafana web ui (get address & details)
    cmds: 
      - |
        GRAFANA_IP=$(kubectl get svc grafana -n observe -o jsonpath='{.spec.clusterIP}')
        GRAFANA_PORT=3045
        GRAFANA_PASS=$(kubectl get secret grafana -n observe -o jsonpath="{.data.admin-password}" | base64 --decode)
        echo "Grafana Dashboard URL: http://${GRAFANA_IP}:${GRAFANA_PORT}"
        echo "Username: admin"
        echo "Password: ${GRAFANA_PASS}"
 
  observe-loki-update-local:
    dir: $DEV_PATH/observe
    desc: upgrade the loki helm pods on observe ns
    cmds:
      - helm upgrade loki grafana/loki -n $MONITOR_NAMESPACE -f ./values/loki.yaml

  observe-prom-update-local:
    dir: $DEV_PATH/observe
    desc: upgrade the prometheus pods on observe ns
    cmds:
      - helm upgrade prometheus prometheus-community/prometheus -n $MONITOR_NAMESPACE -f ./values/prom.yaml
  
  verne-helm-setup-local:
    desc: install verne helm package
    cmds: 
      - helm install verne verne/verne -n verne -f ./dev-values.yaml

  verne-get-ip:
    desc: get verne broker load balancer IP to access 
    cmds:
      - kubectl get svc vernemq-broker -n verne -o jsonpath='{.spec.clusterIP}' 

  _run-k6-test:
    dir: ./scripts/
    desc: load test verneMQTT k3s with k6 [ARGS- duration(secs), vus]
    cmds:
      - bash -c "./k6-run.sh {{.CLI_ARGS}}"

  k6-build:
    dir: ./services/k6
    desc: rebuild the k6 source image from Dockerfile
    cmds:
      - docker build --no-cache -t k6-verne:latest

  k6-image-push:  
    desc: push the k6 verne image to dockerhub [ARGS- version]
    cmds:
      - docker tag k6-verne:latest sardinesszsz/k6-verne:{{.CLI_ARGS}}
      - docker push sardinesszsz/k6-verne:{{.CLI_ARGS}}
    
  k6-github-act-local:
    dir: ../
    desc: run & test github actions CI/CD work locally to push K6 image to dockerhub (dependency - act)
    cmds:
      - act -P act -P ubuntu-latest=catthehacker/ubuntu:act-latest --secret-file .github.env --bind /var/run/docker.sock:/var/run/docker.sock -j build-k6-local

  listener-build:
    dir: ./services/listener
    desc: rebuild the listener source image from Dockerfile
    cmds:
      - docker build --no-cache -t verne-listener:latest .

  listener-push:
    desc: push the listener image to dockerhub [ARGS- version]
    cmds: 
      - docker tag verne-listener:latest sardinesszsz/verne-listener:{{.CLI_ARGS}}
      - docker push sardinesszsz/verne-listener:{{.CLI_ARGS}}

  listener-github-act-local:
    dir: ../
    desc: run & test github actions CI/CD work locally to push lustener image to dockerhub (dependency - act)
    cmds:
      - act -P act -P ubuntu-latest=catthehacker/ubuntu:act-latest --secret-file .github.env --bind /var/run/docker.sock:/var/run/docker.sock -j build-listener-local

  directrunner-pipeline-run:
    dir: ./services/gcp/pipeline
    desc: beam directrunner local pipeline (pubsub -> beam -> bigtable)
    cmds:
      - zsh -i -c "conda activate logcore && python ./pipeline.py --project=$GCP_PROJECT --topic=$GCP_TOPIC --host=$GCP_HOST"

  directrunner-pipeline-view-logs:
    dir: ./services/gcp/pipeline
    desc: read logfile of beam directrunner local (start, stop, messages, etc)
    cmds: 
      - bash -c "tail -f ./pipeline.log"

  gcp-local-run:
    dir: ./services/gcp
    desc: run the gcp local emulator 
    cmds:
      - COMPOSE_BAKE=true docker compose up -d
  
  gcp-local-remove:
    dir: ./services/gcp
    desc: take down gcp local emulator
    cmds: 
      - docker compose down

  gcp-local-init:
    desc: initialize gcp local pubsub(topics) & bigtable(relations)
    cmds:
      - bash -c "./scripts/pubsub-create-topic.sh"
      - zsh -i -c "conda activate logcore && python ./services/gcp/bigtable/init.py"
  
  gcp-local-bigtable-get-relations-list:
    dir: ./services/gcp/bigtable
    desc: get list of test relations on bigtable(local)
    cmds:
      - zsh -i -c "conda activate logcore && python list-relations.py"

  gcp-local-bigtable-query:
    dir: ./services/gcp/bigtable
    desc: get list of test relations on bigtable(local) [ARGS- relation]
    cmds:
      - zsh -i -c "conda activate logcore && python query.py --relation {{.CLI_ARGS}}"

  gcp-local-bigtable-clear-relation:
    dir: ./services/gcp/bigtable
    desc: clear all entries of relations on bigtable(local) [ARGS- relation]
    cmds:
      - zsh -i -c "conda activate logcore && python clear.py --relation {{.CLI_ARGS}}"
 
  gcp-local-pubsub-list-topic:
    desc: delete a specific pubsub topic  
    cmds:
      - curl -X GET "$GCP_LOCAL_BASE_URL/projects/$GCP_PROJECT/topics" 

  gcp-local-pubsub-listen-source:
    dir: ./services/gcp/pubsub
    desc: subsribe and listen to pubsub source(local)
    cmds: 
      - zsh -i -c "conda activate logcore && python sub.py" 

  
      
  # / ======== \
  #   region PROD 
  # \ ======== /
  tf-init:
    dir: ./terraform
    cmds: 
      - terraform init
      
  tf-plan:
    dir: ./terraform
    cmds:
      - terraform plan
   
  tf-apply:
    dir: ./terraform
    cmds:
      - terraform apply 
  
  tf-apply-debug:
    dir: ./terraform
    cmds:
      - TF_LOG=DEBUG TF_LOG_PATH=./tf-debug.log terraform apply -auto-approve
  
  tf-destroy-all:
    dir: ./terraform
    cmds:
      - terraform destroy 
      
  tf-destroy-single:
    dir: ./terraform
    cmds:
      - terraform destroy -target={{.CLI_ARGS}}  
   
  tf-list-res:
    dir: ./terraform
    cmds:
      - terraform state list