version: '3'

dotenv: ['.env']

tasks:
  tester:
    cmds:
      - echo $VERNE_NAMESPACE
      - echo "test"

  ns-create:
    desc: create all required namespaces
    cmds:
      - pwd
      - kubectl apply -f ./k8s/namespaces/

  monitor-deploy:
    desc: helm upgrade the prom-operator pods (afterany config changes)
    cmds: 
      - >
        helm upgrade --install kube-prometheus-stack 
        prometheus-community/kube-prometheus-stack 
        --namespace $MONITOR_NAMESPACE 
        --values ./k8s/monitor/prometheus-operator.yaml

  monitor-destroy:
    desc: helm destroy the monitoring stack
    cmds:
      - helm uninstall kube-prometheus-stack --namespace $MONITOR_NAMESPACE

  grafana-expose:
    desc: export grafana on port 3000 to monitor the k8s pods & metrics
    cmds:
      - |
        export POD_NAME=$(kubectl --namespace $MONITOR_NAMESPACE get pod -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=kube-prometheus-stack" -oname)
        kubectl --namespace $MONITOR_NAMESPACE port-forward $POD_NAME 3000 

  verne-deploy:
    desc: deploy verneMQTT cluster
    deps: [ns-create]
    cmds: 
      - kubectl apply -f ./k8s/vernemq/verne-roles.yaml
      - kubectl apply -f ./k8s/vernemq/verne-depl.yaml
      - kubectl apply -f ./k8s/vernemq/verne-service.yaml

  verne-destroy:
    desc: destroy the verneMQTT & related pods
    cmds: 
      - kubectl delete namespace verne
  
  verne-update:
    desc: update (rolling restart) for verne ns - when making config changes
    cmds:
      - kubectl apply -f ./k8s/vernemq/verne-depl.yaml
      - kubectl rollout restart deployment -n verne

  verne-check-updates:
    desc: updates history for verneMQ namespace
    cmds:
      - kubectl rollout history deployment -n verne

  verne-get-ip:
    desc: get verne broker load balancer IP to access 
    cmds:
      - kubectl get svc vernemq-broker -n verne -o jsonpath='{.spec.clusterIP}'

  verne-set-configs:
    dir: ./k8s/config
    desc: set configmap for verne namespace
    cmds: 
      - kubectl apply -f ./verne-conf.yaml

  verne-get-configs:
    dir: ./scripts
    desc: get current set configmap for verne namespace
    cmds:
      - bash -c "./verne-get-configs.sh"

  verne-mqtt-pub:
    dir: ./scripts
    desc: publish MQTT messages to verne k3s cluster [ARGS - (relation, count)]
    cmds:
      - zsh -i -c "./mqttpub.sh {{.CLI_ARGS}}"

  k6-test:
    dir: ./services/k6/
    desc: load test vernMQTT k3s with k6 [ARGS- control from taskfile]
    env:
      RELATION: weather_sensor
      DURATION: 10
      INTERVAL: 1
      VUS: 5
    cmds:
      - VERNE_IP=$(kubectl get svc -n verne vernemq-broker -o jsonpath='{.spec.clusterIP}')
      - RELATION=$RELATION DURATION=$DURATION INTERVAL=$INTERVAL VERNE_IP=$VERNE_IP k6 run --vus $VUS --duration ${DURATION}s pub.js 

  listener-build:
    dir: ./services/listener
    desc: rebuild the listener source image from Dockerfile
    cmds:
      - docker build --no-cache -t verne-listener:latest .

  listener-push:
    desc: push the listener image to dockerhub [ARGS- version]
    cmds: 
      - docker tag verne-listener:latest sardinesszsz/verne-listener:{{.CLI_ARGS}}
      - docker push sardinesszsz/verne-listener:{{.CLI_ARGS}}

  listener-github-act-local:
    dir: ../
    desc: run & test github actions CI/CD work locally to push lustener image to dockerhub (dependency - act)
    cmds:
      - act -P act -P ubuntu-latest=catthehacker/ubuntu:act-latest --secret-file .github.env --bind /var/run/docker.sock:/var/run/docker.sock -j build-local

  directrunner-pipeline-run:
    dir: ./services/gcp/pipeline
    desc: beam directrunner local pipeline (pubsub -> beam -> bigtable)
    cmds:
      - zsh -i -c "conda activate logcore && python ./pipeline.py --project=$GCP_PROJECT --topic=$GCP_TOPIC --host=$GCP_HOST"

  directrunner-pipeline-view-logs:
    dir: ./services/gcp/pipeline
    desc: read logfile of beam directrunner local (start, stop, messages, etc)
    cmds: 
      - bash -c "tail -f ./pipeline.log"

  gcp-local-run:
    dir: ./services/gcp
    desc: run the gcp local emulator 
    cmds:
      - COMPOSE_BAKE=true docker compose up -d
  
  gcp-local-remove:
    dir: ./services/gcp
    desc: take down gcp local emulator
    cmds: 
      - docker compose down

  gcp-local-init:
    dir: ./services/gcp
    desc: initialize gcp local pubsub(topics) & bigtable(relations)
    cmds:
      - bash -c ""./pubsub/create-topic.sh
      - zsh -i -c "conda activate logcore && python ./bigtable/init.py"
  
  gcp-local-bigtable-get-relations-list:
    dir: ./services/gcp/bigtable
    desc: get list of test relations on bigtable(local)
    cmds:
      - zsh -i -c "conda activate logcore && python list-relations.py"

  gcp-local-bigtable-query:
    dir: ./services/gcp/bigtable
    desc: get list of test relations on bigtable(local) [ARGS- relation]
    cmds:
      - zsh -i -c "conda activate logcore && python query.py --relation {{.CLI_ARGS}}"

  gcp-local-bigtable-clear-relation:
    dir: ./services/gcp/bigtable
    desc: clear all entries of relations on bigtable(local) [ARGS- relation]
    cmds:
      - zsh -i -c "conda activate logcore && python clear.py --relation {{.CLI_ARGS}}"
 
  gcp-local-pubsub-list-topic:
    desc: delete a specific pubsub topic  
    cmds:
      - curl -X GET "$GCP_LOCAL_BASE_URL/projects/$GCP_PROJECT/topics" 

  gcp-local-pubsub-listen-source:
    dir: ./services/gcp/pubsub
    desc: subsribe and listen to pubsub source(local)
    cmds: 
      - zsh -i -c "conda activate logcore && python sub.py" 